{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code was created referring to the code below.\n",
    "#### https://www.kaggle.com/daniel601/pytorch-fasterrcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "'''\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n",
      "                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import cv2\n",
    "\n",
    "\n",
    "imgs_dir = list(sorted(glob('D:/PyTorch/kaggle_train_fasterrcnn/dataset_face_mask/images/*.png')))\n",
    "labels_dir = list(sorted(glob(\"D:/PyTorch/kaggle_train_fasterrcnn/dataset_face_mask/annotations/*.xml\")))\n",
    "\n",
    "class dataset(Dataset) :\n",
    "    def __init__(self, imgs, labels) :\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        x = cv2.imread(self.imgs[index])\n",
    "        x = self.transform(x).to(self.device)\n",
    "        \n",
    "        y = dict()\n",
    "        with open(self.labels[index]) as f :\n",
    "            data = f.read()\n",
    "            soup = BeautifulSoup(data, 'xml')\n",
    "            data = soup.find_all('object')\n",
    "            \n",
    "            box = []\n",
    "            label = []\n",
    "            for obj in data :\n",
    "                xmin = int(obj.find('xmin').text)\n",
    "                ymin = int(obj.find('ymin').text)\n",
    "                xmax = int(obj.find('xmax').text)\n",
    "                ymax = int(obj.find('ymax').text)\n",
    "                \n",
    "                label_ = 0\n",
    "                if obj.find('name').text == 'with_mask' :\n",
    "                    label_ = 1\n",
    "                elif obj.find('name').text == 'mask_weared_incorrect' :\n",
    "                    label_ = 2\n",
    "                \n",
    "                box.append([xmin, ymin, xmax, ymax])\n",
    "                label.append(label_)\n",
    "                \n",
    "            box = torch.FloatTensor(box)\n",
    "            label = torch.IntTensor(label)\n",
    "            \n",
    "            y['image_id'] = torch.FloatTensor([index]).to(device)\n",
    "            y[\"boxes\"] = box.to(device)\n",
    "            y[\"labels\"] = torch.as_tensor(label, dtype=torch.int64)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "def collate_fn(batch) : \n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data = dataset(imgs_dir, labels_dir)\n",
    "train_data = DataLoader(train_data, batch_size = 1, shuffle = True,\n",
    "                       collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습된 모델로 사용 : 학습 시간 아끼자.\n",
    "#load pre-trained model\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "def get_model(output_shape) :\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, output_shape)\n",
    "    \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "model = get_model(3).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr = 0.001, momentum = 0.9, weight_decay = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#학습\n",
    "#train\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model.train()\n",
    "num_epoch = 50\n",
    "\n",
    "# for epoch in range(num_epoch) :\n",
    "#     epoch_loss = 0\n",
    "    \n",
    "#     for imgs, annotations in tqdm(train_data):\n",
    "#         imgs = list(img.to(device) for img in imgs)\n",
    "#         annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "#         predict = model(imgs, annotations)\n",
    "#         losses = sum(loss for loss in predict.values())        \n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step() \n",
    "#         epoch_loss += losses\n",
    "        \n",
    "#     print(epoch+1, '/', num_epoch, ' : {:.5f}'.format(epoch_loss))\n",
    "#     if epoch_loss < 0.1 :\n",
    "#         print('early stop')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of memory 때문에 학습한 거 잃을까봐 일단 저장\n",
    "#because of out of memory, save the model first\n",
    "# torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "model = get_model(3).to(device)\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0af2c20f3e60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#배치 사이즈 조정해서 재실행 => out of memory 방지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Resize the batch-size to prevent out of memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m valid_data = DataLoader(valid_data, batch_size = 4, shuffle = True,\n\u001b[0;32m     10\u001b[0m                        collate_fn = collate_fn)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#결과 확인\n",
    "#get result\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#배치 사이즈 조정해서 재실행 => out of memory 방지\n",
    "#Resize the batch-size to prevent out of memory\n",
    "valid_data = dataset(imgs_dir, labels_dir)\n",
    "valid_data = DataLoader(valid_data, batch_size = 4, shuffle = True,\n",
    "                       collate_fn = collate_fn)\n",
    "\n",
    "def plot_img(img, predict, annotation) :\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    img = img.cpu().data\n",
    "    \n",
    "    ax[0].imshow(img.permute(1, 2, 0)) #rgb, w, h => w, h, rgb\n",
    "    ax[1].imshow(img.permute(1, 2, 0))\n",
    "    ax[0].set_title(\"real\")\n",
    "    ax[1].set_title(\"predict\")\n",
    "    \n",
    "    for box in annotation[\"boxes\"] :\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        ax[0].add_patch(rect)\n",
    "        \n",
    "    for box in predict[\"boxes\"] :\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad() :\n",
    "    for imgs, annotations in valid_data:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        \n",
    "        for i in range(3) :\n",
    "            plot_img(imgs[i], preds[i], annotations[i])\n",
    "            #plot_img(imgs[i], annotations[i])\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
